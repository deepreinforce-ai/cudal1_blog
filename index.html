<!DOCTYPE html><html><head><meta http-equiv="Content-Security-Policy" content="default-src 'self' 'unsafe-inline' 'unsafe-eval' data: blob: https://cdnjs.cloudflare.com https://cdn.jsdelivr.net https://code.jquery.com https://unpkg.com https://d3js.org https://threejs.org https://cdn.plot.ly https://stackpath.bootstrapcdn.com https://maps.googleapis.com https://cdn.tailwindcss.com https://ajax.googleapis.com https://kit.fontawesome.com https://cdn.datatables.net https://maxcdn.bootstrapcdn.com https://code.highcharts.com https://tako-static-assets-production.s3.amazonaws.com https://www.youtube.com https://fonts.googleapis.com https://fonts.gstatic.com https://pfst.cf2.poecdn.net https://puc.poecdn.net https://i.imgur.com https://wikimedia.org https://*.icons8.com https://*.giphy.com https://picsum.photos https://images.unsplash.com; frame-src 'self' https://www.youtube.com https://trytako.com; child-src 'self'; manifest-src 'self'; worker-src 'self'; upgrade-insecure-requests; block-all-mixed-content;"><meta http-equiv="Content-Security-Policy" content="default-src 'self' 'unsafe-inline' 'unsafe-eval' data: blob: https://cdnjs.cloudflare.com https://cdn.jsdelivr.net https://code.jquery.com https://unpkg.com https://d3js.org https://threejs.org https://cdn.plot.ly https://stackpath.bootstrapcdn.com https://maps.googleapis.com https://cdn.tailwindcss.com https://ajax.googleapis.com https://kit.fontawesome.com https://cdn.datatables.net https://maxcdn.bootstrapcdn.com https://code.highcharts.com https://tako-static-assets-production.s3.amazonaws.com https://www.youtube.com https://fonts.googleapis.com https://fonts.gstatic.com https://pfst.cf2.poecdn.net https://puc.poecdn.net https://i.imgur.com https://wikimedia.org https://*.icons8.com https://*.giphy.com https://picsum.photos https://images.unsplash.com; frame-src 'self' https://www.youtube.com https://trytako.com; child-src 'self'; manifest-src 'self'; worker-src 'self'; upgrade-insecure-requests; block-all-mixed-content;">
  <meta charset="utf-8">
  <meta name="description" content="CUDA-L1 leverages contrastive reinforcement learning to automatically optimize CUDA kernels, achieving up to 449x speedup. Official website of 'CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning'">
  <meta name="keywords" content="CUDA-L1, CUDA Optimization, Reinforcement Learning, GPU Programming">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</title>

  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-XXXXXXXXXX');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css">
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer="" src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .highlight-box {
      background-color: #e8f4f8;
      border-left: 4px solid #0066cc;
      padding: 1.5rem;
      margin: 2rem 0;
      border-radius: 0 5px 5px 0;
    }

    .result-card {
      background-color: #f5f5f5;
      padding: 1.5rem;
      border-radius: 8px;
      text-align: center;
      border: 1px solid #ddd;
      margin-bottom: 1rem;
    }

    .result-number {
      font-size: 2.5rem;
      font-weight: bold;
      color: #0066cc;
    }

    .result-label {
      font-size: 0.9rem;
      color: #666;
      margin-top: 0.5rem;
    }

    .logo-img {
      height: 40px;
      width: auto;
    }

    /* Title gradient effect */
    .gradient-title {
      background: linear-gradient(135deg, #0066cc 0%, #00a6ff 50%, #0066cc 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    /* Subsection title styling with left border and background */
    .subsection-title {
      background-color: #f5f5f5;
      border-left: 4px solid #0066cc;
      padding: 0.75rem 1.5rem;
      margin: 2rem 0 1rem 0;
      border-radius: 0 5px 5px 0;
    }

    /* Image styling */
    .figure-container {
      text-align: center;
      margin: 1rem 0;
    }

    .figure-container img {
      max-width: 100%;
      height: auto;
      box-shadow: 0 2px 2px rgba(0, 0, 0, 0.1);
      border-radius: 2px;
    }
   /* Code block styling */
  .code-block {
    position: relative;
    margin: 1.5rem 0;
    border-radius: 8px;
    overflow: hidden;
    background-color: #1e1e1e;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
  }

  .code-header {
    background-color: #2d2d2d;
    padding: 0.5rem 1rem;
    font-size: 0.875rem;
    color: #a0a0a0;
    display: flex;
    justify-content: space-between;
    align-items: center;
    border-bottom: 1px solid #3e3e3e;
  }

  .code-language {
    font-weight: 600;
    color: #61dafb;
  }

  .copy-button {
    background-color: #3e3e3e;
    border: 1px solid #4e4e4e;
    color: #a0a0a0;
    padding: 0.25rem 0.75rem;
    border-radius: 4px;
    font-size: 0.75rem;
    cursor: pointer;
    transition: all 0.2s;
  }

  .copy-button:hover {
    background-color: #4e4e4e;
    color: #ffffff;
  }

   /* Previous styles remain the same until code-content */

  .code-content {
    padding: 1rem;
    overflow-x: auto;
    overflow-y: auto; /* Enable vertical scrolling */
    background-color: #1e1e1e;
    max-height: 400px; /* Fixed maximum height */
  }

  /* Add custom scrollbar styling for dark theme */
  .code-content::-webkit-scrollbar {
    width: 8px;
    height: 8px;
  }

  .code-content::-webkit-scrollbar-track {
    background: #2d2d2d;
    border-radius: 4px;
  }

  .code-content::-webkit-scrollbar-thumb {
    background: #4e4e4e;
    border-radius: 4px;
  }

  .code-content::-webkit-scrollbar-thumb:hover {
    background: #5e5e5e;
  }

  .code-content pre {
    margin: 0;
    font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
    font-size: 0.9rem;
    line-height: 1.6;
    color: #d4d4d4;
    background-color: #1e1e1e; /* Dark background for pre */
    padding: 0;
  }

  .code-content code {
    color: #d4d4d4;
    background-color: #1e1e1e; /* Dark background for code */
    padding: 0;
    display: block;
  }

  /* Override any inherited styles */
  .code-block pre,
  .code-block code {
    background-color: #1e1e1e !important;
    color: #d4d4d4 !important;
  }

  /* Syntax highlighting colors */
  .hljs-keyword { color: #569cd6; }
  .hljs-built_in { color: #4ec9b0; }
  .hljs-type { color: #4ec9b0; }
  .hljs-literal { color: #569cd6; }
  .hljs-number { color: #b5cea8; }
  .hljs-string { color: #ce9178; }
  .hljs-comment { color: #6a9955; font-style: italic; }
  .hljs-function { color: #dcdcaa; }
  .hljs-class { color: #4ec9b0; }
  .hljs-variable { color: #9cdcfe; }
  .hljs-params { color: #9cdcfe; }

  /* Code comparison styling */
  .code-comparison {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 1rem;
    margin: 1.5rem 0;
  }

  .code-comparison .code-block {
    margin: 0;
  }

  @media (max-width: 768px) {
    .code-comparison {
      grid-template-columns: 1fr;
    }
  }

  /* Performance badge */
  .performance-badge {
    position: absolute;
    top: 0.5rem;
    right: 0.5rem;
    background: linear-gradient(135deg, #00a6ff 0%, #0066cc 100%);
    color: white;
    padding: 0.25rem 0.75rem;
    border-radius: 20px;
    font-size: 0.85rem;
    font-weight: bold;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
  }
  </style>
  <script>
  // Copy code functionality
function copyCode(button) {
  const codeBlock = button.closest('.code-block');
  const code = codeBlock.querySelector('pre code').textContent;

  navigator.clipboard.writeText(code).then(() => {
    const originalText = button.textContent;
    button.textContent = 'Copied!';
    button.style.backgroundColor = '#0066cc';
    button.style.color = 'white';

    setTimeout(() => {
      button.textContent = originalText;
      button.style.backgroundColor = '';
      button.style.color = '';
    }, 2000);
  });
}
  </script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://deep-reinforce.com">
        <img src="assets/header.png" alt="DeepReinforce" class="logo-img">
      </a>
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start">
        <!-- Home button removed -->
      </div>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container">

        <div class="container has-text-centered" style="margin-bottom: 0rem;">

          <h1 class="title is-1 publication-title gradient-title" style="margin-top: -2rem;">
            CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">DeepReinforce Team</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">July 21, 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.14111" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- GitHub Link -->
              <span class="link-block">
                <a href="https://github.com/deepreinforce-ai/CUDA-L1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- Demo Link -->
              <span class="link-block">
                <a href="https://deep-reinforce.com/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-play"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>

              <!-- Speedup Image -->
            <div class="figure-container" style="padding-top: 0rem; margin-top: 0rem; margin-bottom: 0rem;">
              <img src="assets/gpu_config_comparison.png" alt="CUDA-L1 Speedup Results" style="transform: scale(0.85); transform-origin: center top; margin-top: 1rem; margin-bottom: -5rem;">
              <div class="subcaptions" style="text-align: center; margin-top: 0px; margin-bottom: -2rem; line-height: 1.4;">
              <p style="margin: 0 0 0px 0;"><small>Average speedup across different optimization configurations on 5 types of GPU architectures.</small></p>
            </div>
            </div>
            </div>
            </div>
          </div>


        </div>
      </div>
    </div>
  </section>

  <section class="section", style="padding-top: 0rem;">
    <div class="container is-max-desktop">

      <!-- Introduction -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>

          <div class="content has-text-left">
            <h4 class="title is-5 subsection-title">The GPU Crisis and How AI Might Save Us</h4>
            <p>
              Let's face it - we're in the middle of a GPU shortage crisis. Everyone wants GPUs for their AI projects. The demand is through the roof, and prices are absolutely insane - a single H100 can cost over $30,000, and good luck even finding one in stock.
            </p>
            <p>
              For most companies and researchers, buying more GPUs simply isn't an option. The only realistic solution? We need to squeeze every bit of performance from the GPUs we already have.
            </p>

            <h4 class="title is-5 subsection-title">The Old Way: Manual CUDA Optimization Hell</h4>
            <p>
              If you've ever tried optimizing CUDA code, you know the pain. It's like solving a massive puzzle where you're constantly tweaking memory access patterns, adjusting thread blocks, and running endless profiling tests. Engineers spend weeks or months on this stuff, and it's honestly exhausting.
            </p>

            <h4 class="title is-5 subsection-title">What if LLMs Could Do This For Us?</h4>
            <p>
              Here's where things get interesting. Recent LLM models - think DeepSeek-R1 and OpenAI's o1 - are getting pretty good at writing code. And here's the kicker: CUDA optimization has a super clear reward signal - speed! Your code either runs faster or it doesn't. That's perfect for training RL.
            </p>
            <p>
              Imagine this: instead of you pulling your hair out trying different optimizations, an AI could generate thousands of variations, test them all, and learn what works. It might even discover tricks that humans never thought of!
            </p>

            <h4 class="title is-5 subsection-title">Introducing CUDA-L1</h4>
            <p>
              So we built CUDA-L1, which uses something we call "contrastive reinforcement learning." Think of it like this: instead of just trying random stuff, our AI compares different CUDA versions side-by-side and learns why some are faster than others. It's like having a coach that shows you good vs. bad examples until you get it. And we found CUDA-L1 excels at:
            </p>
            <ul style="text-align: left;">
              <li><strong>Discover optimization techniques</strong> – techniques like memory coalescing, loop unrolling, operation fusion. Some of these are well-known, others are rarely used.</li>
              <li><strong>Figure out the perfect combo</strong> – like a chef who knows exactly which spices work together, it combines optimizations in ways that maximize performance.</li>
              <li><strong>Learn the "rules" of CUDA</strong> – like how some optimizations multiply each other's effects, or how you need to apply certain "gatekeeper" techniques first before others will work.</li>
              <li><strong>Spot hidden problems</strong> – sometimes it rejects optimizations that look good on paper but actually slow things down due to sneaky issues like CPU-GPU sync overhead.</li>
            </ul>
          </div>
        </div>
      </div>
      <!--/ Introduction -->

    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <!-- How CUDA-L1 Works -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">How CUDA-L1 Works ?</h2>

          <div class="content has-text-left">
            <h4 class="title is-5 subsection-title">The Problem: Why Can't Current LLMs Write Good CUDA?</h4>
            <p>
              Ask any AI to write CUDA code and you'll likely get something that doesn't compile, crashes, or runs painfully slow. The reason is simple: these models barely saw any quality CUDA code during training. It's like asking someone who's only read cooking blogs to become a chef.
            </p>

            <h4 class="title is-5 subsection-title">CUDA-L1: A Three-Step Recipe</h4>
            <p>
              We built CUDA-L1 with a three-stage pipeline: supervised learning (learn the basics), self-supervised learning (practice until perfect), and contrastive reinforcement learning (compete for speed).
            </p>

            <!-- Pipeline Image -->
            <div class="figure-container" style="padding-top: 0rem; margin-top: 1rem; margin-bottom: -2rem;">
              <img src="assets/pipeline.png" alt="CUDA-L1 Pipeline", style="transform: scale(0.98); transform-origin: center top;">
            </div>

            <h5 class="title is-6 subsection-title">Stage 1: Learning the Basics with Data Augmentation</h5>
            <p>
              First, we needed to fix the data shortage problem. We took existing CUDA code and created variations of it - expanding the model's exposure to different CUDA patterns. This supervised fine-tuning phase has one goal: make sure the AI can write CUDA code that actually compiles and runs correctly.
            </p>

            <h5 class="title is-6 subsection-title">Stage 2: Practice Makes Perfect with Self-Supervised Learning</h5>
            <p>
              Next, we let the model generate its own CUDA code, test it, and learn from what works. The model generates thousands of code samples, we automatically test each one, and only the successful implementations get fed back for more training. No speed optimization yet - just making sure the code works reliably.
            </p>

            <h5 class="title is-6 subsection-title">Stage 3: The Speed Revolution - Contrastive Reinforcement Learning</h5>
            <p>
              This is where CUDA-L1 becomes special. Traditional RL would just assign scores to generated code and hope the model figures out why some implementations are faster. That's like grading exams without showing students the correct answers. Instead, we do something radically different. Look at this actual prompt we use:
            </p>
          </div>
          <div class="highlight-box">
          <h5 class="title is-6">CUDA Kernel Optimization Prompt:</h5>
          <div class="content has-text-left">
          <h5 class="title is-6">We show the AI multiple CUDA implementations WITH their speed scores:</h5>
            <ul style="text-align: left;">
              <li>"Here's kernel_v1 that achieves 1.2x speedup"</li>
              <li>"Here's kernel_v2 that achieves 2.8x speedup"</li>
              <li>"Here's kernel_v3 that achieves 1.5x speedup"</li>
            </ul>
            <p style="text-align: left; margin-top: 1rem;">
              <strong>Then we ask three critical questions:</strong>
            </p>
            <ol style="text-align: left;">
              <li>Performance Analysis: "Why is kernel_v2 so much faster? What optimizations did it use that the others didn't?"</li>
              <li>Algorithm Design: "Based on this analysis, what optimization strategy would work even better?"</li>
              <li>Code Implementation: "Now write a kernel that beats them all."</li>
            </ol>
        </div>

          <div class="content has-text-left">
            <p>
              The magic happens because the AI can directly see and reason about performance differences. It's not guessing in the dark - it's learning from concrete examples of what makes CUDA code fast.
            </p>
          </div>

        </div>
      </div>
      <!--/ How CUDA-L1 Works -->
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Mitigating Reward Hacking -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Mitigating Reward Hacking in RL Training</h2>

        <div class="content has-text-left">
          <p>
            Reinforcement learning is notorious for exhibiting reward hacking behaviors, where models exploit system vulnerabilities to achieve higher rewards while generating outputs that deviate from the intended objectives. In our experiments, we discovered that over 30% of generated implementations attempted some form of reward hacking.
          </p>

          <h4 class="title is-5 subsection-title">Reward Hacking Cases</h4>
          <p>
            During our initial training procedure, we identified three major categories of reward hacking behaviors:
          </p>

          <h5>1. Improper Timing Measurement</h5>
          <p>
            KernelBench measures execution time by recording timing events on the main CUDA stream as follows:
          </p>

          <div class="code-block">
            <div class="code-header">
              <span class="code-language">CUDA - Vulnerable Timing</span>
              <button class="copy-button" onclick="copyCode(this)">Copy</button>
            </div>
            <div class="code-content">
              <pre><code><span class="hljs-function">start_event</span>.<span class="hljs-function">record</span>(original_model_stream)
<span class="hljs-function">model</span>(*inputs)
<span class="hljs-function">end_event</span>.<span class="hljs-function">record</span>(original_model_stream)
torch.cuda.<span class="hljs-function">synchronize</span>(device=device)</code></pre>
            </div>
          </div>

          <div class="warning-box" style="background-color: #fff3cd;">
            <p><strong>Warning:</strong> This vulnerability led to artificial 18× speedups that were entirely fake - the actual computation performance was unchanged!</p>
          </div>

          <p>
            However, RL-generated code exploits this by creating additional CUDA streams that execute asynchronously. Since KernelBench only monitors the main stream, it fails to capture the actual execution time of operations running on parallel streams. This vulnerability is significant: in our initial implementation, we find that 82 out of 250 (32.8%) RL-generated implementations exploit this timing loophole to appear faster than they actually are, leading to an overall speedup of 18×. To address this issue, prompt engineering alone is insufficient. Our fix synchronizes all CUDA streams:
          </p>

          <div class="code-block">
            <div class="code-header">
              <span class="code-language">CUDA - Fixed Timing</span>
              <button class="copy-button" onclick="copyCode(this)">Copy</button>
            </div>
            <div class="code-content">
              <pre><code><span class="hljs-keyword">if</span> custom_contain_new_streams:
    <span class="hljs-keyword">for</span> stream <span class="hljs-keyword">in</span> custom_model_streams:
        custom_model_stream.<span class="hljs-function">wait_stream</span>(stream)
<span class="hljs-function">end_event</span>.<span class="hljs-function">record</span>(custom_model_stream)</code></pre>
            </div>
          </div>

          <h5>2. Lazy Evaluation</h5>
            <p>
              Another critical vulnerability discovered by community users on GitHub involves lazy evaluation. When calling custom_model(*inputs), the output isn't necessarily materialized or computed immediately. Instead, the actual computation can be deferred until the correctness check phase when torch.allclose() is invoked, allowing submissions to pass correctness checks while avoiding proper timing measurement.
            </p>

            <div class="code-block">
              <div class="code-header">
                <span class="code-language">Python - Lazy Evaluation Exploit</span>
                <button class="copy-button" onclick="copyCode(this)">Copy</button>
              </div>
              <div class="code-content">
                <pre><code>
                <span class="hljs-keyword">class</span> LazyMatmul(torch.Tensor):
                  def __new__(cls, A, B):
                    result = torch.Tensor._make_subclass(cls, torch.empty(<span class="hljs-number">0</span>))
                    result.A = A
                    result.B = B
                    result._shape = (A.size(<span class="hljs-number">0</span>), B.size(<span class="hljs-number">1</span>))
                    return result

                  def materialize(self):
                    """Trigger actual computation"""
                    return torch.matmul(self.A, self.B)

                <span class="hljs-keyword">class</span> ModelNew(nn.Module):
                    <span class="hljs-keyword">def</span> forward(<span class="hljs-keyword">self</span>, A, B):
                        <span class="hljs-keyword">return</span> LazyMatmul(A, B)  <span class="hljs-comment"># Returns lazy object</span></code></pre>
                  </div>
            </div>

            <div class="warning-box" style="background-color: #fff3cd;">
              <p><strong>Warning:</strong> This exploit returns a lazy tensor subclass that defers all computation until the correctness check, completely bypassing the timing measurement!</p>
            </div>

            <p>
              To mitigate this issue, we enforce a comprehensive validation check that ensures all outputs are fully materialized before ending the time measurement. This involves verifying five critical conditions: the output must be a tensor, must be a standard torch.Tensor (not a subclass), must be on the correct device, must have allocated memory, and the corresponding storage must be valid.
            </p>

            <div class="code-block">
              <div class="code-header">
                <span class="code-language">Python - Materialization Validation</span>
                <button class="copy-button" onclick="copyCode(this)">Copy</button>
              </div>
              <div class="code-content">
                <pre><code>
            <span class="hljs-comment"># Check 1: Must be a tensor</span>
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isinstance(out, torch.Tensor):
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>, <span class="hljs-string">f&quot;{prefix} is not a tensor: {type(out)}&quot;</span>

            <span class="hljs-comment"># Check 2: Must be standard torch.Tensor, not a subclass</span>
            <span class="hljs-keyword">if</span> type(out).__name__ <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">'Tensor'</span>, <span class="hljs-string">'Parameter'</span>]:
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>, <span class="hljs-string">f&quot;{prefix} is {type(out).__name__}, not standard torch.Tensor&quot;</span>

            <span class="hljs-comment"># Check 3: Must be on correct device</span>
            <span class="hljs-keyword">if</span> out.device != device:
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>, <span class="hljs-string">f&quot;{prefix} on wrong device: {out.device} (expected {device})&quot;</span>

            <span class="hljs-comment"># Check 4: Must have allocated storage</span>
            storage_size = out.untyped_storage().size()
            <span class="hljs-keyword">if</span> storage_size == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>, <span class="hljs-string">f&quot;{prefix} has no allocated storage (likely lazy)&quot;</span>

            <span class="hljs-comment"># Check 5: Storage pointer must be valid</span>
            ptr = out.data_ptr()
            <span class="hljs-keyword">if</span> ptr == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>, <span class="hljs-string">f&quot;{prefix} storage pointer is null (likely lazy)&quot;</span></code></pre>
              </div>
            </div>


          <h5>3. Hyperparameter Manipulation</h5>
          <p>
            In KernelBench, each computational task is associated with specific hyper- parameters, including <em>batch_size</em>, <em>dim</em>, <em>in_features</em> dimension, <em>out_features</em> dimension, <em>scaling_factor</em>, and others. The RL agent learned to exploit these parameters by generating code that artificially reduces their values, thereby achieving superficial speedup improvements that do not reflect genuine optimization performance.
          </p>
          <h5>4. Result Caching</h5>
          <p>
            The RL agent developed strategies to cache computational results across evaluation batches based on input addresses. When another input’s address matches a cached one, it returns the cached output. The following code snippet gives an illustration:
          </p>

          <div class="code-block">
            <div class="code-header">
              <span class="code-language">Python - Caching Example</span>
              <button class="copy-button" onclick="copyCode(this)">Copy</button>
            </div>
            <div class="code-content">
              <pre><code>cache_key = x.data_ptr()
<span class="hljs-keyword">if</span> cache_key <span class="hljs-keyword">in</span> self.cache:
    <span class="hljs-keyword">return</span> self.cache[cache_key]</code></pre>
            </div>
          </div>

          <h4 class="title is-5 subsection-title">Defense Strategies</h4>
          <p>
            To combat these sophisticated reward hacking behaviors, we developed a multi-layered defense system combining automated detection, continuous learning, and mathematical constraints.
          </p>

          <h5>1. Adversarial Reward Checking Model</h5>
          <p>
            We deployed DeepSeek-R1 as an adversarial checker that analyzes generated code for potential exploits. The model achieves over 60% detection accuracy through pattern analysis, semantic understanding, and anomaly detection. When speedups exceed suspicious thresholds (e.g., >10× for simple operations), it triggers multi-stage verification including output consistency checks, memory usage analysis, and GPU utilization metrics.
          </p>

          <h5>2. Dynamic Hacking-Case Database</h5>
          <p>
            A continuously updated database containing 500+ unique hacking patterns helps identify new exploits. For each generated implementation, we retrieve similar hacking cases using AST-based code analysis and performance profiling. This contextual information improves detection accuracy by 25% and allows us to track how hacking strategies evolve across training iterations.
          </p>

          <h5>3. Reward Smoothing and Normalization</h5>
          <p>
            To prevent over-optimization for extreme cases, we apply mathematical constraints on rewards:
          </p>

          <div style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 8px; margin: 1rem 0;">
            <p style="text-align: center; font-family: 'Times New Roman', serif; font-size: 1.1rem;">
              <em>r</em><sub>normalized</sub> = (<em>r</em> - μ) / σ
              <br>
              <em>r</em><sub>smooth</sub> = clip(<em>r</em><sub>normalized</sub>, -<em>k</em>, <em>k</em>)
            </p>
          </div>

          <p>
            Here, μ and σ are rolling statistics updated every 100 iterations, and <em>k</em> = 1.5 represents the maximum reasonable speedup. For suspicious high-reward cases with low confidence, we apply additional dampening based on consistency metrics and alignment with known optimization patterns.
          </p>

          <div class="highlight-box">
            <p><strong>Key Takeaway:</strong> The arms race between the RL agent and our detection systems highlights the importance of robust evaluation frameworks. Our multi-layered approach has proven effective at maintaining training integrity while still allowing genuine breakthrough optimizations to be rewarded appropriately.</p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Mitigating Reward Hacking -->
  </div>
</section>

  <section class="section">
    <div class="container is-max-desktop">

       <!-- Results -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Evaluations</h2>

          <div class="content has-text-left">
            <p>
              We tested CUDA-L1 on KernelBench, a comprehensive benchmark suite with three difficulty levels:
            </p>
            <ul>
              <li><strong>Level 1:</strong> A subset that contains simple operations (like matrix multiply)</li>
              <li><strong>Level 2:</strong> A subset that contains operator sequences (like attention mechanisms)</li>
              <li><strong>Level 3:</strong> A subset that contains complex ML tasks (like full transformer layers)</li>
            </ul>
            <p>
              And we use <strong>All Levels</strong> to denote the full dataset containing all three levels.
            </p>

            <p>
              To perform a comprehensive evaluation on the generated code, we perform the following comparisons:
            </p>

            <p>
              <strong>I) Default</strong>: This compares the CUDA-L1 generated code with the reference code by KernelBench.
            </p>

            <p>
              <strong>II) Torch Compile</strong>: This compares the CUDA-L1 generated code with the reference code enhanced by torch.compile with default settings. Torch.compile applies graph-level optimizations including operator fusion, memory planning, and kernel selection to accelerate PyTorch models through just-in-time compilation.
            </p>

            <p>
              <strong>III) Torch Compile Reduce Overhead</strong>: This compares the CUDA-L1 generated code with the reference code enhanced by torch.compile with reduce-overhead mode enabled. This mode minimizes the compilation overhead by caching compiled graphs more aggressively and reducing recompilation frequency, making it particularly suitable for inference workloads with static shapes.
            </p>

            <p>
              <strong>IV) CUDA Graph</strong>: Since KernelBench does not provide official CUDA Graph implementations, we employ Claude 4 to generate CUDA Graph-augmented code for each reference implementation. CUDA Graphs capture a series of CUDA kernels and their dependencies into a single graph structure that can be launched with minimal CPU overhead, eliminating the need for repeated kernel launch commands and significantly reducing CPU-GPU synchronization costs.
            </p>
          </div>

          <div class="table-container">
            <table class="table is-striped is-fullwidth">
              <caption><strong>Performance comparison across different configurations on KernelBench on A100</strong></caption>
              <thead>
                <tr>
                  <th>Configuration</th>
                  <th>DataType</th>
                  <th>Mean</th>
                  <th>Max</th>
                  <th>75%</th>
                  <th>50%</th>
                  <th>25%</th>
                  <th>Success↑<br><small># out of total</small></th>
                  <th>Speedup↑<br><small>>1.01x out of total</small></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>Default</em></td>
                  <td><strong>All Levels</strong></td>
                  <td><strong>3.12×</strong></td>
                  <td><strong>120×</strong></td>
                  <td>2.25×</td>
                  <td>1.42×</td>
                  <td>1.17×</td>
                  <td>249/250</td>
                  <td>226/250</td>
                </tr>
                <tr>
                  <td><em>Torch Compile</em></td>
                  <td><strong>All Levels</strong></td>
                  <td>2.77×</td>
                  <td>69.0×</td>
                  <td>2.55×</td>
                  <td>1.72×</td>
                  <td>1.14×</td>
                  <td>249/250</td>
                  <td>203/250</td>
                </tr>
                <tr>
                  <td><em>Torch Compile RO</em></td>
                  <td><strong>All Levels</strong></td>
                  <td>2.88×</td>
                  <td>80.1×</td>
                  <td>2.48×</td>
                  <td>1.67×</td>
                  <td>1.13×</td>
                  <td>249/250</td>
                  <td>200/250</td>
                </tr>
                <tr>
                  <td><em>CUDA Graph</em></td>
                  <td><strong>All Levels</strong></td>
                  <td>2.81×</td>
                  <td>97.9×</td>
                  <td>1.83×</td>
                  <td>1.20×</td>
                  <td>0.954×</td>
                  <td>249/250</td>
                  <td>147/229</td>
                </tr>
              </tbody>
            </table>
            <div class="content has-text-left has-text-grey is-size-7">
              <p>
                • RO = Reduce Overhead<br>
                • Success and Speedup indicate the number of successful benchmarks out of the total for each level
              </p>
            </div>
          </div>

          <div class="content has-text-left">
            <h4 class="title is-5 subsection-title">Generalization of A100-Optimized Kernels to Other GPU Architectures</h4>
            <p>
              We trained CUDA-L1 on NVIDIA A100s, but what if you're using a different GPU? Good news: the optimizations transfer remarkably well. We tested the same A100-optimized kernels on:
            </p>
          </div>

          <div class="table-container">
            <table class="table is-striped is-fullwidth">
              <caption><strong>Mean speedup across different configurations and GPU devices</strong></caption>
              <thead>
                <tr>
                  <th>Configuration</th>
                  <th>A100</th>
                  <th>3090</th>
                  <th>H100</th>
                  <th>H20</th>
                  <th>L40</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>Default</em></td>
                  <td><strong>3.12×</strong></td>
                  <td>2.51×</td>
                  <td><strong>3.85×</strong></td>
                  <td>2.38×</td>
                  <td>3.13×</td>
                </tr>
                <tr>
                  <td><em>Torch Compile</em></td>
                  <td>2.77×</td>
                  <td>2.58×</td>
                  <td>2.74×</td>
                  <td><strong>2.89×</strong></td>
                  <td>2.85×</td>
                </tr>
                <tr>
                  <td><em>Torch Compile RO</em></td>
                  <td>2.88×</td>
                  <td>2.61×</td>
                  <td>2.77×</td>
                  <td>2.82×</td>
                  <td><strong>2.89×</strong></td>
                </tr>
                <tr>
                  <td><em>CUDA Graph</em></td>
                  <td>2.81×</td>
                  <td><strong>3.34×</strong></td>
                  <td>2.23×</td>
                  <td>2.20×</td>
                  <td><strong>3.98×</strong></td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="content has-text-left">
            <p>
              The results show fascinating patterns across different GPU architectures:
            </p>
            <ul>
              <li><strong>H100 excels with default CUDA-L1 optimizations (3.85×)</strong>, showing that our base optimizations work particularly well on the latest datacenter GPUs.</li>
              <li><strong>Consumer GPUs (RTX 3090) benefit more from CUDA Graphs (3.34×)</strong>, likely due to their different memory hierarchies and lower CPU-GPU communication overhead.</li>
              <li><strong>Kernels working on L40 shows exceptional performance with CUDA Graphs (3.98×)</strong>, the highest speedup we observed across all configurations.</li>
              <li><strong>Optimized kernels show consistent performance across all GPUs</strong>, ranging from 2.20× to 3.89×, demonstrating strong generalization capability.</li>
            </ul>
            <p>
              These results confirm that CUDA-L1's optimization patterns are fundamental enough to benefit any modern GPU architecture, though different GPUs may favor different optimization strategies. This suggests exciting opportunities for GPU-specific training in future versions of CUDA-L1.
            </p>
          </div>

        </div>
      </div>
      <!--/ Results -->
    </div>
  </section>

  <section class="section">
  <div class="container is-max-desktop">

    <!-- Case Study -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Case Studies</h2>

        <div class="content has-text-left">
          <p>
            Let's dive deep into three examples to understand what CUDA-L1 actually does:
          </p>

          <h4 class="title is-5 subsection-title">Case 1: Diagonal Matrix Multiplication - 64× Faster</h4>
          <p>
            This task performs matrix multiplication between a diagonal matrix (represented by its diagonal elements) and a dense matrix, both with dimension N=4096.
          </p>

          <!-- Code comparison for diagonal matrix multiplication -->
          <div class="code-comparison">
            <div class="code-block">
              <div class="code-header">
                <span class="code-language">Python - Reference Implementation</span>
                <button class="copy-button" onclick="copyCode(this)">Copy</button>
              </div>
              <div class="code-content">
                <pre><code><span class="hljs-keyword">class</span> <span class="hljs-class">Model</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-function">forward</span>(<span class="hljs-params">self, A, B</span>):
        <span class="hljs-comment"># A: (N,) - 1D tensor of shape N</span>
        <span class="hljs-comment"># B: (N, M) - 2D tensor of shape N x M</span>
        <span class="hljs-comment"># torch.diag(A): (N, N) - creates diagonal matrix from A</span>
        <span class="hljs-comment"># Result: (N, N) @ (N, M) = (N, M)</span>
        <span class="hljs-keyword">return</span> torch.diag(A) @ B</code></pre>
              </div>
            </div>

            <div class="code-block">
              <div class="code-header">
                <span class="code-language">Python - CUDA-L1 Optimized</span>
                <button class="copy-button" onclick="copyCode(this)">Copy</button>
              </div>
              <div class="performance-badge">64× faster</div>
              <div class="code-content">
                <pre><code><span class="hljs-keyword">class</span> <span class="hljs-class">Model</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-function">forward</span>(<span class="hljs-params">self, A, B</span>):
        <span class="hljs-keyword">return</span> A.unsqueeze(<span class="hljs-number">1</span>) * B</code></pre>
              </div>
            </div>
          </div>

          <p>
            The optimized implementation leverages PyTorch's broadcasting mechanism brilliantly. Instead of creating a full N×N diagonal matrix (which would be mostly zeros), it simply reshapes the diagonal vector A from (N,) to (N, 1) and uses broadcasting to multiply each row of B by the corresponding element of A.
          </p>
          <p>
            The benefits are substantial:
          </p>
          <ul>
            <li>Memory: O(1) extra memory instead of O(N²)</li>
            <li>Computation: O(NM) operations instead of O(N²M)</li>
            <li>Result: 64× speedup!</li>
          </ul>
          <p>
            What's remarkable is that CUDA-L1 discovered this algebraic simplification on its own through RL exploration. By testing semantically equivalent implementations, it learned to identify patterns where computationally expensive operations can be replaced with more efficient alternatives.
          </p>

          <h4 class="title is-5 subsection-title">Case 2: LSTM Networks - 3.4× Faster</h4>
<p>
  For a classical LSTM neural network (Level 3, Task 35), CUDA-L1 achieved a 3.4× speedup by applying three key optimizations:
</p>
<ol>
  <li><strong>CUDA Graphs:</strong> Captures the entire LSTM computation sequence into a replayable graph structure</li>
  <li><strong>Memory Contiguity:</strong> Ensures all tensors maintain contiguous memory layouts</li>
  <li><strong>Static Tensor Reuse:</strong> Pre-allocates tensors and reuses them across forward passes</li>
</ol>

<!-- Code comparison for LSTM -->
<div class="code-comparison">
  <div class="code-block">
    <div class="code-header">
      <span class="code-language">Python - Reference Implementation</span>
      <button class="copy-button" onclick="copyCode(this)">Copy</button>
    </div>
    <div class="code-content">
      <pre><code><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-keyword">class</span> <span class="hljs-class">Model</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-function">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, num_layers, output_size, dropout=<span class="hljs-number">0.0</span></span>):
        <span class="hljs-string">"""
        Initialize the LSTM model.
        :param input_size: The number of expected features in the input `x`
        :param hidden_size: The number of features in the hidden state `h`
        :param num_layers: Number of recurrent layers
        :param output_size: The number of output features
        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`
        """</span>
        <span class="hljs-keyword">super</span>(Model, self).__init__()
        <span class="hljs-comment"># Initialize hidden state with random values</span>
        self.h0 = torch.randn((num_layers, batch_size, hidden_size))
        self.c0 = torch.randn((num_layers, batch_size, hidden_size))
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="hljs-literal">True</span>, dropout=dropout, bidirectional=<span class="hljs-literal">False</span>)
        self.fc = nn.Linear(hidden_size, output_size)

    <span class="hljs-keyword">def</span> <span class="hljs-function">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-string">"""
        Forward pass through the LSTM model.
        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :return: The output tensor, shape (batch_size, sequence_length, output_size)
        """</span>
        self.h0 = self.h0.to(x.device)
        self.c0 = self.h0.to(x.device)  <span class="hljs-comment"># BUG: This should be self.c0.to(x.device)</span>

        <span class="hljs-comment"># Forward propagate LSTM</span>
        out, state = self.lstm(x, (self.h0, self.c0))  <span class="hljs-comment"># out: tensor of shape (batch_size, seq_length, hidden_size)</span>

        <span class="hljs-comment"># Decode the hidden state of the last time step</span>
        out = self.fc(out[:, <span class="hljs-number">-1</span>, :])  <span class="hljs-comment"># out: tensor of shape (batch_size, output_size)</span>

        <span class="hljs-keyword">return</span> state[<span class="hljs-number">0</span>]

<span class="hljs-comment"># Test code</span>
batch_size = <span class="hljs-number">10</span>
sequence_length = <span class="hljs-number">512</span>
input_size = <span class="hljs-number">128</span>
hidden_size = <span class="hljs-number">256</span>
num_layers = <span class="hljs-number">6</span>
output_size = <span class="hljs-number">10</span>
dropout = <span class="hljs-number">0.0</span>

<span class="hljs-keyword">def</span> <span class="hljs-function">get_inputs</span>():
    <span class="hljs-keyword">return</span> [torch.randn(batch_size, sequence_length, input_size)]

<span class="hljs-keyword">def</span> <span class="hljs-function">get_init_inputs</span>():
    <span class="hljs-keyword">return</span> [input_size, hidden_size, num_layers, output_size, dropout]</code></pre>
    </div>
  </div>

  <div class="code-block">
    <div class="code-header">
      <span class="code-language">Python - CUDA-L1 Optimized</span>
      <button class="copy-button" onclick="copyCode(this)">Copy</button>
    </div>
    <div class="performance-badge">3.4× faster</div>
    <div class="code-content">
      <pre><code><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.cuda <span class="hljs-keyword">as</span> cuda

<span class="hljs-keyword">class</span> <span class="hljs-class">ModelNew</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-function">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, num_layers, output_size, dropout=<span class="hljs-number">0.0</span></span>):
        <span class="hljs-string">"""
        Initialize the LSTM model with three core optimization techniques.

        Color coding:
        - 🔵 BLUE: CUDA Graphs optimization
        - 🟢 GREEN: Memory Contiguity optimization
        - 🟠 ORANGE: Static Tensor Reuse optimization
        """</span>
        <span class="hljs-keyword">super</span>(ModelNew, self).__init__()

        <span class="hljs-comment"># Initialize hidden states as buffers</span>
        self.register_buffer(<span class="hljs-string">'h0'</span>, torch.randn((num_layers, batch_size, hidden_size)))
        self.register_buffer(<span class="hljs-string">'c0'</span>, torch.randn((num_layers, batch_size, hidden_size)))

        <span class="hljs-comment"># Use PyTorch's optimized LSTM implementation</span>
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=<span class="hljs-literal">True</span>,
            dropout=dropout,
            bidirectional=<span class="hljs-literal">False</span>
        )

        self.fc = nn.Linear(hidden_size, output_size)

        <span class="hljs-comment"># 🔵 CUDA GRAPHS: Variables for graph capture and replay</span>
        self.graph = <span class="hljs-literal">None</span>
        self.graph_ready = <span class="hljs-literal">False</span>
        self.input_shape = <span class="hljs-literal">None</span>

        <span class="hljs-comment"># 🟠 STATIC TENSOR REUSE: Pre-allocated tensors for graph execution</span>
        self.static_input = <span class="hljs-literal">None</span>
        self.static_output = <span class="hljs-literal">None</span>

        <span class="hljs-comment"># 🔵 CUDA GRAPHS: Streams for graph operations</span>
        self.graph_stream = <span class="hljs-literal">None</span>

        <span class="hljs-comment"># Track if we're running on CUDA</span>
        self.is_cuda_available = torch.cuda.is_available()

    <span class="hljs-keyword">def</span> <span class="hljs-function">_initialize_cuda_resources</span>(<span class="hljs-params">self</span>):
        <span class="hljs-string">"""🔵 CUDA GRAPHS: Initialize CUDA stream for graph operations"""</span>
        <span class="hljs-keyword">if</span> self.graph_stream <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            self.graph_stream = cuda.Stream()

    <span class="hljs-keyword">def</span> <span class="hljs-function">_capture_graph</span>(<span class="hljs-params">self, x, result</span>):
        <span class="hljs-string">"""
        🔵 CUDA GRAPHS: Capture the computation graph for replay
        🟠 STATIC TENSOR REUSE: Create static tensors for graph capture
        """</span>
        <span class="hljs-comment"># 🟠 STATIC TENSOR REUSE: Clone tensors for static allocation</span>
        self.static_input = x.clone()
        self.static_output = result.clone()

        <span class="hljs-comment"># 🔵 CUDA GRAPHS: Capture the computation graph</span>
        <span class="hljs-keyword">with</span> torch.cuda.stream(self.graph_stream):
            self.graph = cuda.CUDAGraph()
            <span class="hljs-keyword">with</span> cuda.graph(self.graph):
                <span class="hljs-comment"># Operations to capture in the graph</span>
                static_out, _ = self.lstm(self.static_input, (self.h0, self.c0))

                <span class="hljs-comment"># 🟢 MEMORY CONTIGUITY: Ensure contiguous memory layout</span>
                static_last = static_out[:, <span class="hljs-number">-1</span>, :].contiguous()

                self.static_output.copy_(self.fc(static_last))

        <span class="hljs-comment"># Wait for graph capture to complete</span>
        torch.cuda.synchronize()

        <span class="hljs-comment"># Mark graph as ready for use</span>
        self.graph_ready = <span class="hljs-literal">True</span>

    <span class="hljs-keyword">def</span> <span class="hljs-function">_standard_forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-string">"""Standard forward pass with memory contiguity optimization"""</span>

        <span class="hljs-comment"># 🟢 MEMORY CONTIGUITY: Ensure input is contiguous</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> x.is_contiguous():
            x = x.contiguous()

        <span class="hljs-comment"># Forward pass through LSTM</span>
        out, _ = self.lstm(x, (self.h0, self.c0))

        <span class="hljs-comment"># 🟢 MEMORY CONTIGUITY: Make last output contiguous for optimal memory access</span>
        last_out = out[:, <span class="hljs-number">-1</span>, :].contiguous()

        <span class="hljs-keyword">return</span> self.fc(last_out)

    <span class="hljs-keyword">def</span> <span class="hljs-function">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-string">"""
        Forward pass through the LSTM model with three optimization techniques.

        Optimization flow:
        1. 🔵 CUDA GRAPHS: Check if we can use the captured graph (fast path)
        2. 🟠 STATIC TENSOR REUSE: Use pre-allocated tensors for graph replay
        3. 🟢 MEMORY CONTIGUITY: Ensure optimal memory layout throughout
        """</span>

        <span class="hljs-comment"># 🔵 CUDA GRAPHS: Fast path - use captured graph if available</span>
        <span class="hljs-keyword">if</span> (x.is_cuda <span class="hljs-keyword">and</span>
            self.graph_ready <span class="hljs-keyword">and</span>
            x.shape == self.input_shape):

            <span class="hljs-comment"># 🟠 STATIC TENSOR REUSE: Copy to pre-allocated tensor with non-blocking transfer</span>
            self.static_input.copy_(x, non_blocking=<span class="hljs-literal">True</span>)

            <span class="hljs-comment"># 🔵 CUDA GRAPHS: Replay the captured graph</span>
            self.graph.replay()

            <span class="hljs-comment"># Return the output from static buffer</span>
            <span class="hljs-keyword">return</span> self.static_output.clone()

        <span class="hljs-comment"># Standard execution path</span>
        <span class="hljs-keyword">with</span> torch.no_grad():
            result = self._standard_forward(x)

            <span class="hljs-comment"># 🔵 CUDA GRAPHS: Initialize graph on first CUDA input</span>
            <span class="hljs-keyword">if</span> x.is_cuda <span class="hljs-keyword">and</span> self.is_cuda_available <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.graph_ready:
                <span class="hljs-keyword">try</span>:
                    <span class="hljs-comment"># Store the current input shape</span>
                    self.input_shape = x.shape

                    <span class="hljs-comment"># 🔵 CUDA GRAPHS: Initialize CUDA resources</span>
                    self._initialize_cuda_resources()

                    <span class="hljs-comment"># 🔵 CUDA GRAPHS + 🟠 STATIC TENSOR REUSE: Capture the graph</span>
                    self._capture_graph(x, result)

                <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
                    <span class="hljs-comment"># If graph capture fails, continue without it</span>
                    self.graph_ready = <span class="hljs-literal">False</span>

            <span class="hljs-keyword">return</span> result

            <span class="hljs-comment"># Hyperparameters from the reference implementation</span>
            batch_size = <span class="hljs-number">10</span>
            sequence_length = <span class="hljs-number">512</span>
            input_size = <span class="hljs-number">128</span>
            hidden_size = <span class="hljs-number">256</span>
            num_layers = <span class="hljs-number">6</span>
            output_size = <span class="hljs-number">10</span>
            dropout = <span class="hljs-number">0.0</span></code></pre>
                </div>
              </div>
            </div>

            <p>
              The optimized implementation uses color-coded emojis to highlight the three optimization techniques:
              <span style="color: #007bff;">🔵 CUDA Graphs</span>,
              <span style="color: #28a745;">🟢 Memory Contiguity</span>, and
              <span style="color: #fd7e14;">🟠 Static Tensor Reuse</span>.
              The results reveal something: CUDA Graphs is essential for any meaningful speedup. Without it, no combination of other optimizations provides any benefit. But once CUDA Graphs is enabled, the additional optimizations provide incremental improvements, with all three together achieving the best 3.42× speedup.
            </p>

          <h4 class="title is-5 subsection-title">Case 3: 3D Convolution Pipeline - 120× Faster</h4>
<p>
  The most impressive speedup came from a 3D operation pipeline: transposed convolution, average pooling, clamping, softmax, and element-wise multiplication. CUDA-L1 applied four optimizations:
</p>
<ol>
  <li><strong>Mathematical Short-Circuit:</strong> Detects when min_value equals 0.0 and skips the entire computation</li>
  <li><strong>Pre-allocated Tensors:</strong> Creates zero tensors during initialization</li>
  <li><strong>Direct Shape Matching:</strong> Provides a fast path for standard input shapes</li>
  <li><strong>Pre-computed Parameters:</strong> Stores convolution parameters during initialization</li>
</ol>

<!-- Code comparison for Conv3D -->
<div class="code-comparison">
  <div class="code-block">
    <div class="code-header">
      <span class="code-language">Python - Reference Implementation</span>
      <button class="copy-button" onclick="copyCode(this)">Copy</button>
    </div>
    <div class="code-content">
      <pre><code><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-keyword">class</span> <span class="hljs-class">Model</span>(nn.Module):
    <span class="hljs-string">"""
    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.
    """</span>
    <span class="hljs-keyword">def</span> <span class="hljs-function">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p</span>):
        <span class="hljs-keyword">super</span>(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.norm = nn.GroupNorm(groups, out_channels)
        self.dropout = nn.Dropout(dropout_p)
        self.min_value = min_value
        self.max_value = max_value

    <span class="hljs-keyword">def</span> <span class="hljs-function">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.conv(x)
        x = self.norm(x)
        x = torch.min(x, torch.tensor(self.min_value))
        x = torch.clamp(x, min=self.min_value, max=self.max_value)
        x = self.dropout(x)
        <span class="hljs-keyword">return</span> x

<span class="hljs-comment"># Hyperparameters</span>
batch_size = <span class="hljs-number">128</span>
in_channels = <span class="hljs-number">3</span>
out_channels = <span class="hljs-number">16</span>
depth, height, width = <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>
kernel_size = <span class="hljs-number">3</span>
groups = <span class="hljs-number">8</span>
min_value = <span class="hljs-number">0.0</span>
max_value = <span class="hljs-number">1.0</span>
dropout_p = <span class="hljs-number">0.2</span>

<span class="hljs-keyword">def</span> <span class="hljs-function">get_inputs</span>():
    <span class="hljs-keyword">return</span> [torch.randn(batch_size, in_channels, depth, height, width)]

<span class="hljs-keyword">def</span> <span class="hljs-function">get_init_inputs</span>():
    <span class="hljs-keyword">return</span> [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]</code></pre>
    </div>
  </div>

  <div class="code-block">
    <div class="code-header">
      <span class="code-language">Python - CUDA-L1 Optimized</span>
      <button class="copy-button" onclick="copyCode(this)">Copy</button>
    </div>
    <div class="performance-badge">120× faster</div>
    <div class="code-content">
      <pre><code><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-comment"># Hyperparameters</span>
batch_size = <span class="hljs-number">128</span>
in_channels = <span class="hljs-number">3</span>
out_channels = <span class="hljs-number">16</span>
depth, height, width = <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>
kernel_size = <span class="hljs-number">3</span>
groups = <span class="hljs-number">8</span>
min_value = <span class="hljs-number">0.0</span>
max_value = <span class="hljs-number">1.0</span>
dropout_p = <span class="hljs-number">0.2</span>

<span class="hljs-keyword">class</span> <span class="hljs-class">ModelNew</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-function">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p</span>):
        <span class="hljs-keyword">super</span>(ModelNew, self).__init__()
        <span class="hljs-comment"># Store the original layers for parameter compatibility</span>
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.norm = nn.GroupNorm(groups, out_channels)
        self.dropout = nn.Dropout(dropout_p)
        self.min_value = min_value
        self.max_value = max_value
        self.dropout_p = dropout_p

        <span class="hljs-comment"># 🔵 TECH 1: Mathematical Short-Circuit Optimization</span>
        <span class="hljs-comment"># Detects when min_value=0.0 to skip entire computation</span>
        self.use_optimized_path = (min_value == <span class="hljs-number">0.0</span>)

        <span class="hljs-comment"># 🟠 TECH 4: Pre-computed Convolution Parameters</span>
        <span class="hljs-comment"># Extract and store conv parameters once during initialization</span>
        <span class="hljs-keyword">if</span> isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size, kernel_size)
        <span class="hljs-keyword">else</span>:
            self.kernel_size = kernel_size
        self.stride = self.conv.stride
        self.padding = self.conv.padding
        self.dilation = self.conv.dilation

        <span class="hljs-comment"># 🟠 TECH 4: Pre-compute output dimensions for standard input</span>
        self.out_depth = ((depth + <span class="hljs-number">2</span> * self.padding[<span class="hljs-number">0</span>] - self.dilation[<span class="hljs-number">0</span>] * (self.kernel_size[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>) - <span class="hljs-number">1</span>) // self.stride[<span class="hljs-number">0</span>]) + <span class="hljs-number">1</span>
        self.out_height = ((height + <span class="hljs-number">2</span> * self.padding[<span class="hljs-number">1</span>] - self.dilation[<span class="hljs-number">1</span>] * (self.kernel_size[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span>) - <span class="hljs-number">1</span>) // self.stride[<span class="hljs-number">1</span>]) + <span class="hljs-number">1</span>
        self.out_width = ((width + <span class="hljs-number">2</span> * self.padding[<span class="hljs-number">2</span>] - self.dilation[<span class="hljs-number">2</span>] * (self.kernel_size[<span class="hljs-number">2</span>] - <span class="hljs-number">1</span>) - <span class="hljs-number">1</span>) // self.stride[<span class="hljs-number">2</span>]) + <span class="hljs-number">1</span>

        <span class="hljs-comment"># Standard output shape for the default batch size</span>
        self.standard_shape = (batch_size, out_channels, self.out_depth, self.out_height, self.out_width)

        <span class="hljs-comment"># 🟣 TECH 2: Pre-allocated Zero Tensors</span>
        <span class="hljs-comment"># Create zero tensors once to avoid allocation overhead</span>
        <span class="hljs-keyword">if</span> self.use_optimized_path:
            self.register_buffer(<span class="hljs-string">'zero_output_float32'</span>,
                               torch.zeros(self.standard_shape, dtype=torch.float32),
                               persistent=<span class="hljs-literal">False</span>)
            self.register_buffer(<span class="hljs-string">'zero_output_float16'</span>,
                               torch.zeros(self.standard_shape, dtype=torch.float16),
                               persistent=<span class="hljs-literal">False</span>)
            self.register_buffer(<span class="hljs-string">'zero_output_bfloat16'</span>,
                               torch.zeros(self.standard_shape, dtype=torch.bfloat16),
                               persistent=<span class="hljs-literal">False</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-function">calculate_output_shape</span>(<span class="hljs-params">self, input_shape</span>):
        <span class="hljs-string">"""Calculate the output shape of the convolution operation."""</span>
        batch_size, _, d, h, w = input_shape

        <span class="hljs-comment"># 🟠 TECH 4: Use precomputed parameters</span>
        <span class="hljs-comment"># Avoid repeated attribute lookups</span>
        out_d = ((d + <span class="hljs-number">2</span> * self.padding[<span class="hljs-number">0</span>] - self.dilation[<span class="hljs-number">0</span>] * (self.kernel_size[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>) - <span class="hljs-number">1</span>) // self.stride[<span class="hljs-number">0</span>]) + <span class="hljs-number">1</span>
        out_h = ((h + <span class="hljs-number">2</span> * self.padding[<span class="hljs-number">1</span>] - self.dilation[<span class="hljs-number">1</span>] * (self.kernel_size[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span>) - <span class="hljs-number">1</span>) // self.stride[<span class="hljs-number">1</span>]) + <span class="hljs-number">1</span>
        out_w = ((w + <span class="hljs-number">2</span> * self.padding[<span class="hljs-number">2</span>] - self.dilation[<span class="hljs-number">2</span>] * (self.kernel_size[<span class="hljs-number">2</span>] - <span class="hljs-number">1</span>) - <span class="hljs-number">1</span>) // self.stride[<span class="hljs-number">2</span>]) + <span class="hljs-number">1</span>

        <span class="hljs-keyword">return</span> (batch_size, self.conv.out_channels, out_d, out_h, out_w)

    <span class="hljs-keyword">def</span> <span class="hljs-function">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-comment"># 🔵 TECH 1: Mathematical Short-Circuit - Main optimization</span>
        <span class="hljs-comment"># Skip all computation when we know result will be zeros</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.use_optimized_path:
            <span class="hljs-comment"># Standard path for non-optimized cases</span>
            x = self.conv(x)
            x = self.norm(x)
            x = torch.minimum(x, torch.tensor(self.min_value, device=x.device))
            x = torch.clamp(x, min=self.min_value, max=self.max_value)
            x = self.dropout(x)
            <span class="hljs-keyword">return</span> x

        <span class="hljs-comment"># Optimized path when min_value == 0.0</span>
        <span class="hljs-comment"># Since min(x, 0) followed by clamp(0, 1) always produces zeros</span>

        <span class="hljs-comment"># 🟢 TECH 3: Direct Shape Matching</span>
        <span class="hljs-comment"># Fast path for standard input dimensions</span>
        <span class="hljs-keyword">if</span> x.shape == (batch_size, in_channels, depth, height, width):
            <span class="hljs-comment"># 🟣 TECH 2: Use pre-allocated tensors</span>
            <span class="hljs-comment"># Return pre-allocated zeros matching input dtype</span>
            <span class="hljs-keyword">if</span> x.dtype == torch.float32:
                <span class="hljs-keyword">return</span> self.zero_output_float32
            <span class="hljs-keyword">elif</span> x.dtype == torch.float16:
                <span class="hljs-keyword">return</span> self.zero_output_float16
            <span class="hljs-keyword">elif</span> x.dtype == torch.bfloat16:
                <span class="hljs-keyword">return</span> self.zero_output_bfloat16
            <span class="hljs-keyword">else</span>:
                <span class="hljs-comment"># Fallback for other dtypes</span>
                <span class="hljs-keyword">return</span> torch.zeros(self.standard_shape, device=x.device, dtype=x.dtype)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-comment"># For non-standard input shapes, calculate output shape</span>
            output_shape = self.calculate_output_shape(x.shape)
            <span class="hljs-keyword">return</span> torch.zeros(output_shape, device=x.device, dtype=x.dtype)

<span class="hljs-comment"># Color Legend:</span>
<span class="hljs-comment"># 🔵 TECH 1: Mathematical Short-Circuit (Blue) - Skips computation when min_value=0</span>
<span class="hljs-comment"># 🟣 TECH 2: Pre-allocated Tensors (Purple) - Pre-allocates zero tensors</span>
<span class="hljs-comment"># 🟢 TECH 3: Direct Shape Matching (Green) - Fast path for standard shapes</span>
<span class="hljs-comment"># 🟠 TECH 4: Pre-computed Parameters (Orange) - Pre-computes conv parameters</span></code></pre>
    </div>
  </div>
</div>

<p>
  The optimized implementation uses color-coded emojis to highlight the four optimization techniques:
  <span style="color: #007bff;">🔵 Mathematical Short-Circuit</span>,
  <span style="color: #9b59b6;">🟣 Pre-allocated Tensors</span>,
  <span style="color: #28a745;">🟢 Direct Shape Matching</span>, and
  <span style="color: #fd7e14;">🟠 Pre-computed Parameters</span>.
</p>

        <div class="content has-text-left">
          <p>
            These case studies reveal the true power of CUDA-L1: it doesn't just apply known optimization tricks, it discovers fundamental mathematical and computational insights that lead to performance improvements. Through reinforcement learning, it explores the vast space of possible implementations and learns principles that even experienced CUDA programmers might miss.
          </p>
        </div>

      </div>
    </div>
    <!--/ Case Study -->
  </div>
  </section>

   <section class="section">
    <div class="container is-max-desktop">

      <!-- Limitations and Challenges -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Limitations and Challenges</h2>

          <div class="content has-text-left">
            <p>
              During the training process, we found that RL is particularly susceptible to reward hacking. We've already identified quite some hacking cases (e.g., exploiting timing measurements & caching results). If you identify any additional reward hacks in the code, we would greatly appreciate you letting us know. You can contact us via email at `research@deep-reinforce.com` or open a GitHub issue at https://github.com/deepreinforce-ai/CUDA-L1
            </p>
          </div>

        </div>
      </div>
      <!--/ Limitations and Challenges -->
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container content is-max-desktop">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{deepreinforce2025cudal1,
  title={CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning},
  author={DeepReinforce Team},
  journal={arXiv preprint arXiv:2507.14111},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          © 2025 DeepReinforce. All rights reserved.
        </p>
        <p>
          Research advancing AI through nature-inspired algorithms
        </p>
      </div>
    </div>
  </footer>

  <script type="text/javascript" src="./static/slick/slick.js"></script>




</body></html>